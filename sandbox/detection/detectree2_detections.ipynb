{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.evaluation.coco_evaluation import instances_to_coco_json\n",
    "\n",
    "from torchgeo.datasets import unbind_samples\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tree_detection_framework.utils.geometric import mask_to_shapely\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ofo-share/repos-amritha/conda/envs/tree-detection-framework2/lib/python3.10/site-packages/kornia/feature/lightglue.py:44: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "from tree_detection_framework.preprocessing.preprocessing import create_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = create_dataloader(\n",
    "    raster_folder_path=\"/ofo-share/scratch-amritha/emerald_point_dtree2/dataset/emerald-point-ortho\",\n",
    "    # raster_folder_path=\"/ofo-share/scratch-derek/tdf-testing/\",\n",
    "    chip_size=512,\n",
    "    chip_stride=400,\n",
    "    batch_size=3,\n",
    "    output_resolution=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_cfg(\n",
    "    base_model: str = \"COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml\",\n",
    "    trains=(\"trees_train\",),\n",
    "    tests=(\"trees_val\",),\n",
    "    update_model=None,\n",
    "    workers=2,\n",
    "    ims_per_batch=2,\n",
    "    gamma=0.1,\n",
    "    backbone_freeze=3,\n",
    "    warm_iter=120,\n",
    "    momentum=0.9,\n",
    "    batch_size_per_im=1024,\n",
    "    base_lr=0.0003389,\n",
    "    weight_decay=0.001,\n",
    "    max_iter=1000,\n",
    "    num_classes=1,\n",
    "    eval_period=100,\n",
    "    out_dir=\"./train_outputs\",\n",
    "    resize=True,\n",
    "):\n",
    "    \"\"\"Set up config object # noqa: D417.\n",
    "\n",
    "    Args:\n",
    "        base_model: base pre-trained model from detectron2 model_zoo\n",
    "        trains: names of registered data to use for training\n",
    "        tests: names of registered data to use for evaluating models\n",
    "        update_model: updated pre-trained model from detectree2 model_garden\n",
    "        workers: number of workers for dataloader\n",
    "        ims_per_batch: number of images per batch\n",
    "        gamma: gamma for learning rate scheduler\n",
    "        backbone_freeze: backbone layer to freeze\n",
    "        warm_iter: number of iterations for warmup\n",
    "        momentum: momentum for optimizer\n",
    "        batch_size_per_im: batch size per image\n",
    "        base_lr: base learning rate\n",
    "        weight_decay: weight decay for optimizer\n",
    "        max_iter: maximum number of iterations\n",
    "        num_classes: number of classes\n",
    "        eval_period: number of iterations between evaluations\n",
    "        out_dir: directory to save outputs\n",
    "    \"\"\"\n",
    "    cfg = get_cfg()\n",
    "    cfg.merge_from_file(model_zoo.get_config_file(base_model))\n",
    "    cfg.DATASETS.TRAIN = trains\n",
    "    cfg.DATASETS.TEST = tests\n",
    "    cfg.DATALOADER.NUM_WORKERS = workers\n",
    "    cfg.SOLVER.IMS_PER_BATCH = ims_per_batch\n",
    "    cfg.SOLVER.GAMMA = gamma\n",
    "    cfg.MODEL.BACKBONE.FREEZE_AT = backbone_freeze\n",
    "    cfg.SOLVER.WARMUP_ITERS = warm_iter\n",
    "    cfg.SOLVER.MOMENTUM = momentum\n",
    "    cfg.MODEL.RPN.BATCH_SIZE_PER_IMAGE = batch_size_per_im\n",
    "    cfg.SOLVER.WEIGHT_DECAY = weight_decay\n",
    "    cfg.SOLVER.BASE_LR = base_lr\n",
    "    cfg.OUTPUT_DIR = out_dir\n",
    "    os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "    if update_model is not None:\n",
    "        cfg.MODEL.WEIGHTS = update_model\n",
    "    else:\n",
    "        cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(base_model)\n",
    "\n",
    "    cfg.SOLVER.IMS_PER_BATCH = ims_per_batch\n",
    "    cfg.SOLVER.BASE_LR = base_lr\n",
    "    cfg.SOLVER.MAX_ITER = max_iter\n",
    "    cfg.MODEL.ROI_HEADS.NUM_CLASSES = num_classes\n",
    "    cfg.TEST.EVAL_PERIOD = eval_period\n",
    "    cfg.RESIZE = resize\n",
    "    cfg.INPUT.MIN_SIZE_TRAIN = 1000\n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ofo-share/repos-amritha/conda/envs/tree-detection-framework2/lib/python3.10/site-packages/rasterio/merge.py:369: NodataShadowWarning: The dataset's nodata attribute is shadowing the alpha band. All masks will be determined by the nodata attribute\n",
      "  temp_src = src.read(\n"
     ]
    }
   ],
   "source": [
    "all_batches = list(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_batches[0]['image'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in dataloader:\n",
    "#     images = batch['image']\n",
    "#     for image in images:\n",
    "#         image = image.permute(1, 2, 0).byte().numpy()\n",
    "#         original_image = image[:, :, :3]\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DefaultBatchPredictor(DefaultPredictor):\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            original_image (np.ndarray): an image of shape (H, W, C) (in BGR order).\n",
    "\n",
    "        Returns:\n",
    "            predictions (dict):\n",
    "                the output of the model for one image only.\n",
    "                See :doc:`/tutorials/models` for details about the format.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():  # https://github.com/sphinx-doc/sphinx/issues/4258\n",
    "\n",
    "            inputs = []\n",
    "            for original_image in batch:\n",
    "                original_image = original_image.permute(1, 2, 0).byte().numpy()\n",
    "                original_image = original_image[:, :, :3]\n",
    "                print(\"Original image shape = \", original_image.shape)\n",
    "                \n",
    "                # Apply pre-processing to image.\n",
    "                if self.input_format == \"RGB\":\n",
    "                    print(\"input format = \", self.input_format)\n",
    "                    # whether the model expects BGR inputs or RGB\n",
    "                    original_image = original_image[:, :, ::-1]\n",
    "                height, width = original_image.shape[:2]\n",
    "                image = self.aug.get_transform(original_image).apply_image(original_image)\n",
    "                image = torch.as_tensor(image.astype(\"float32\").transpose(2, 0, 1))\n",
    "                image.to(self.cfg.MODEL.DEVICE)\n",
    "                print(\"Final image shape = \", image.shape)\n",
    "                input = {\"image\": image, \"height\": height, \"width\": width}\n",
    "                inputs.append(input)\n",
    "\n",
    "            batch_preds = self.model(inputs)\n",
    "            return batch_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = \"/ofo-share/repos-amritha/detectree2-code/230103_randresize_full.pth\"  # Load pretrained weights from local\n",
    "cfg = setup_cfg(update_model=trained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ofo-share/repos-amritha/conda/envs/tree-detection-framework2/lib/python3.10/site-packages/fvcore/common/checkpoint.py:252: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(f, map_location=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "predictor = DefaultBatchPredictor(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original image shape =  (512, 512, 3)\n",
      "Final image shape =  torch.Size([3, 800, 800])\n",
      "Original image shape =  (512, 512, 3)\n",
      "Final image shape =  torch.Size([3, 800, 800])\n",
      "Original image shape =  (512, 512, 3)\n",
      "Final image shape =  torch.Size([3, 800, 800])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ofo-share/repos-amritha/conda/envs/tree-detection-framework2/lib/python3.10/site-packages/torch/functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3609.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "def predict_batch(batch):\n",
    "    images = batch[\"image\"]\n",
    "    batch_preds = predictor(images)\n",
    "\n",
    "    all_geometries = []\n",
    "    all_data_dicts = []\n",
    "\n",
    "    for pred in batch_preds:\n",
    "        instances = pred[\"instances\"].to(\"cpu\")\n",
    "\n",
    "        pred_masks = instances.pred_masks.numpy()\n",
    "        shapely_objects = [mask_to_shapely(pred_mask) for pred_mask in pred_masks]\n",
    "        all_geometries.append(shapely_objects)\n",
    "\n",
    "        scores = instances.scores.numpy()\n",
    "        labels = instances.pred_classes.numpy()\n",
    "        all_data_dicts.append({\"score\": scores, \"labels\": labels})\n",
    "\n",
    "    return all_geometries, all_data_dicts\n",
    "\n",
    "all_geometries, all_data_dicts = predict_batch(all_batches[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_data_dicts[0]['labels'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tree-detection-framework2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
